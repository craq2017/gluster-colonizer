---

- hosts: localhost
  name: "Generate SSH keypairs locally"

  vars:
    current_user: "{{ansible_user|default(lookup('env', 'USER')) }}"

  tasks:

    - name: generate ssh key for current user {{ current_user }}
      user:
        name: "{{ current_user }}"
        generate_ssh_key: yes
        ssh_key_bits: 4096
        ssh_key_type: rsa
        state: present

- hosts: all
  name: "Bootstrap HPE Apollo 4200 nodes for RHS One - General-purpose NAS flavor"

  become: true

  vars_prompt:
    - name: bonding_mode
      prompt: "\n\nThis playbook will configure a network bond of interface ens1f0 and ens2f0. Please type 'lacp' for 802.3ad or 'tlb' or balance-tlb as the bonding mode."
      default: 'lacp'
      private: no

    - name: apply_disk_config
      prompt: "\n\nThis playbook will configure two RAID6 sets, each of 12 hard disks. It will also initialize the NVMe devices. This will DESTROY any data on all those devices. The OS disks are not touched. Select 'yes' if you agree or 'no' if you prepared the RAID sets and NVMe devices yourself as /dev/sdb, /dev/sdc and /dev/nvme0n1, /dev/nvme1n1 respectively."
      default: 'no'
      private: no

  vars:
    disks:
      sdb:
        host: "RAID bus controller: Hewlett-Packard Company Smart Array Gen9 Controllers (rev 01)"
        model: "LOGICAL VOLUME"
        vendor: "HP"
        size: "9.10 TB"
      sdc:
        host: "RAID bus controller: Hewlett-Packard Company Smart Array Gen9 Controllers (rev 01)"
        model: "LOGICAL VOLUME"
        vendor: "HP"
        size: "9.10 TB"
      nvme0n1:
        host: "Non-Volatile memory controller: Intel Corporation PCIe Data Center SSD (rev 01)"
        model: "MT0800KEXUU"
        vendor: null
        size: "745.21 GB"
      nvme1n1:
        host: "Non-Volatile memory controller: Intel Corporation PCIe Data Center SSD (rev 01)"
        model: "MT0800KEXUU"
        vendor: null
        size: "745.21 GB"

  tasks:

    - assert:
        that:
          - bonding_mode == 'lacp' or bonding_mode == 'tlb'
        msg: "Incorrect network bonding mode defined: {{ bonding_mode }}"

    - assert:
        that:
          - apply_disk_config == 'yes' or apply_disk_config == 'no'
        msg: "Invalid response. Please specify either 'yes' or 'no'."

    - name: add generated ssh key to hosts
      authorized_key:
        user: "{{ current_user }}"
        state: present
        key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"

    - name: add sudoers configuration for the current user
      lineinfile:
        path: /etc/sudoers.d/{{ current_user }}
        line: "{{ current_user }}        ALL=(ALL)       NOPASSWD: ALL"
        create: yes
        state: present

    - name: allow non-interactive sudo access
      lineinfile:
        path: /etc/sudoers
        line: "#Defaults requiretty"
        regexp: "^Defaults(.*?)requiretty"
        backrefs: yes
        state: present

    - name: disable DNS lookup in sshd
      lineinfile:
        path: /etc/ssh/sshd_config
        line: "UseDNS no"
        state: present
      notify:
        - reload sshd

    - name: install gluster
      yum: name={{ item }} state=present
      with_items:
        - "@RH-Gluster-Core"
        - "@RH-Gluster-NFS-Ganesha"
        - "@RH-Gluster-Tools"
        - "@scalable-file-systems"
        - nfs-utils
        - vim
        - vim-enhanced
        - net-tools
        - telnet
        - wget
        - mlocate
        - firewalld
        - sysstat
        - net-snmp-utils
        - redhat-support-tool
        - NetworkManager
        - NetworkManager-team
        - NetworkManager-glib
        - teamd
        - gstatus
        - ctdb
      notify:
        - enable firewalld
        - enable networkmanager

    # - name: install colonizer
    #   yum: name={{ item }} state=present
    #   with_items:
    #     - gluster-colonizer
    #     - gluster-zeroconf-avahi
    #   notify:
    #     - enable zeroconf
    #     - allow zeroconf
    #
    # - name: configure avahi daemon to only allow {{ ansible_default_ipv4.alias }}
    #   lineinfile:
    #     line: "allow-interfaces={{ ansible_default_ipv4.alias }}"
    #     path: /etc/avahi/avahi-daemon.conf
    #     insertafter: "\\[server\\]"
    #     state: present

    - name: configure system panic settings
      blockinfile:
        block: |
          kernel.unknown_nmi_panic = 1
          kernel.softlockup_panic = 1
          kernel.nmi_watchdog = 1
        path: /etc/sysctl.d/panic.conf
        create: yes
        state: present

    - name: retrieve NetworkManager connection names for onboard nics
      shell: nmcli -f UUID,DEVICE -t connection show | grep {{ item }}
      changed_when: false
      register: nmcli_conn_show_loms_cmd
      with_items:
        - eno1

    - name: rename NetworkManager connection name for onboard nics
      shell: nmcli connection modify {{ item.stdout.split(':')[0] }} connection.id {{ item.stdout.split(':')[1] }}
      with_items:
        - "{{ nmcli_conn_show_loms_cmd.results }}"

    - name: delete any existing bonding connection
      shell: nmcli connection delete {{ item }}
      ignore_errors: true
      failed_when: false
      changed_when: false
      with_items:
        - bond0
        - bond0-ens1f0
        - bond0-ens2f0
        - ens1f0
        - ens2f0

    - name: delete any existing bonding device
      shell: nmcli device delete {{ item }}
      ignore_errors: true
      failed_when: false
      changed_when: false
      with_items:
        - bond0
        - bond0-ens1f0
        - bond0-ens2f0

    - name: create network team for gluster data traffic
      nmcli:
        type: bond
        conn_name: bond0
        mode: "{{ (bonding_mode == 'lacp') | ternary('802.3ad', 'balance-tlb') }}"
        state: present
      changed_when: false

    - name: add network nics on ens1f0 and ens2f0 to the network team
      nmcli:
        type: bond-slave
        master: bond0
        conn_name: "bond0-{{ item }}"
        ifname: "{{ item }}"
        state: present
      with_items:
        - ens1f0
        - ens2f0
      changed_when: false

    - name: verify bond connection state
      shell: cat /sys/class/net/bond0/bonding/mii_status
      register: bonding_state_cmd
      until: bonding_state_cmd.stdout == 'up'
      retries: 5
      delay: 5
      changed_when: false

    - block:

        - name: enumerate raid controller using hpssacli
          shell: hpssacli controller all show
          register: hpssacli_controller_show_cmd
          changed_when: false

        - set_fact:
            array_controller_slot: "{{ hpssacli_controller_show_cmd.stdout | regex_search('Smart Array .* in Slot ([0-9])', '\\1') | first }}"

        - name: looking for logical drives except OS RAID
          shell: hpssacli controller slot={{ array_controller_slot }} ld all show
          changed_when: false
          register: hpssacli_pd_show_cmd

        - set_fact:
            additional_logical_drives: "{{ hpssacli_pd_show_cmd.stdout | regex_findall('array ([B-Z])', '\\1') | sort(true) }}"

        - name: delete all logical drives except OS RAID
          shell: hpssacli controller slot={{ array_controller_slot }} array {{ item | upper }} delete forced
          changed_when: false
          with_items: "{{ additional_logical_drives }}"

        - name: create RAID configuration template
          blockinfile:
            block: |
              Action= Configure
              Method= Custom

              Controller= SLOT {{ array_controller_slot }}
              PowerMode= MaxPerformance
              ReadCache= 10
              WriteCache= 90
              RebuildPriority= High
              ExpandPriority= Medium
              ParallelSurfaceScanCount= 1
              SurfaceScanMode= Idle
              SurfaceScanDelay= 3
              Latency= Disable
              DriveWriteCache= Disabled
              NoBatteryWriteCache= Disabled
              MNPDelay= 60
              IRPEnable= Disabled
              DPOEnable= Disabled
              ElevatorSortEnable= Enabled
              QueueDepth= Automatic
              PredictiveSpareActivation= Disable

              Array= B
              Drive= 1I:1:13, 1I:1:14, 1I:1:15, 1I:1:16, 1I:1:17, 1I:1:18, 1I:1:19, 1I:1:20, 1I:1:21, 1I:1:22, 1I:1:49, 1I:1:50
              OnlineSpare= No

              LogicalDrive= 2
              RAID= 6
              Sectors= 32
              StripSize= 256
              Caching= Enabled

              Array= C
              Drive= 2I:1:1, 2I:1:2, 2I:1:3, 2I:1:4, 2I:1:5, 2I:1:6, 2I:1:7, 2I:1:8, 2I:1:9, 2I:1:10, 2I:1:11, 2I:1:12
              OnlineSpare= No

              LogicalDrive= 3
              RAID= 6
              Sectors= 32
              StripSize= 256
              Caching= Enabled
            path: /tmp/ssainput.ini
            create: yes

        - name: apply RAID configuration from template
          shell: hpssascripting -i /tmp/ssainput.ini
          changed_when: false

      when: apply_disk_config == 'yes'

    - name: check if disk sizes and naming is as expected
      with_items: "{{ disks.keys() }}"
      assert:
        that:
          - ansible_devices[item]['host'] == disks[item]['host']
          - ansible_devices[item]['model'] == disks[item]['model']
          - ansible_devices[item]['vendor'] == disks[item]['vendor']
          - ansible_devices[item]['size'] == disks[item]['size']
        msg: "The system does not have the required disk layout."

  handlers:

    - name: enable firewalld
      service: name=firewalld state=started enabled=yes

    - name: enable networkmanager
      service: name=NetworkManager state=started enabled=yes

    - name: reload sshd
      service: name=sshd state=reloaded

    - name: enable avahi socket
      systemd: unit=avahi-daemon.socket state=started enabled=yes
      listen: enable zeroconf

    - name: enable avahi service
      systemd: unit=avahi-daemon.service state=started enabled=yes
      listen: enable zeroconf

    - name: allow zeroconf
      firewalld: service=mdns state=enabled permanent=true immediate=true

...
