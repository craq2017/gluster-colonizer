- hosts: gluster_nodes
  become: yes
  any_errors_fatal: True

  tasks:

    - name: Build mntpath variable
      set_fact:
        mntpaths: "{{ mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"

    - name: Build arbiter_mntpath variable
      set_fact:
        arbiter_mntpaths: "{{ arbiter_mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/arbiter-{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-arbiter--{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - shell: /bin/bash -c 'echo "Cleaning up any existing Gluster configurations..." > {{ fifo }}'

    - name: Stop glusterd service
      systemd:
        name: glusterd
        state: stopped

    - name: Kill all Gluster processes
      shell: /bin/bash -c 'pkill -9 {{ item }}'
      with_items:
        - glusterfs
        - glusterfsd
      ignore_errors: yes

    - name: Clear existing Gluster configuration
      file:
        path: "/var/lib/glusterd/{{ item }}"
        state: absent
      with_items:
        - glusterd.info
        - glustershd
        - bitd
        - glusterfind
        - glustershd
        - nfs
        - peers
        - quotad
        - scrub
        - snaps
        - ss_brick
        - vols
      ignore_errors: yes

    - name: Unmount data bricks
      mount:
        name: "{{ item.0.path }}"
        state: "{{ item.1 }}"
      with_nested:
        - "{{ mntpaths }}"
        - ['absent', 'unmounted']

    - name: Unmount arbiter bricks
      mount:
        name: "{{ item.0.path }}"
        state: "{{ item.1 }}"
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - ['absent', 'unmounted']
      when: arbiter == True

      #- name: Deactivate VGs
      #shell: /bin/bash -c 'vgchange -a n --activationmode partial {{ item.vg }}'
      #with_items: "{{ backend_configuration }}"
      #register: result
      #failed_when: "result.rc != 0 and 'not found' not in result.stderr"

    - name: Clear LVM
      lvg:
        vg: "{{ item.vg }}"
        state: absent
        force: yes
      with_items: "{{ backend_configuration }}"

    - name: Clear cache LVM
      lvg:
        vg: "FAST{{ item.vg }}"
        state: absent
        force: yes
      with_items: "{{ backend_configuration }}"

    #TODO: Add a play or enhance the above to clear fast device LVM

    - name: Remove PV
      shell: /bin/bash -c 'pvremove --force {{ item.device }}'
      register: result
      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
      with_items: "{{ backend_configuration }}"

    - name: Remove cache PV
      #FIXME: The 'p' is needed below for nvme drives, but should not be
      #       there for devices with normal /dev/sdX disk paths.
      shell: /bin/bash -c 'pvremove --force {{ item.0 }}p{{ item.1["id"]|int + 1 }}'
      register: result
      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"

    - name: Cleanup data devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item["device"] }}'
      with_items: "{{ backend_configuration }}"
      failed_when: "result.rc != 0 and 'probing initialization failed: No such file or directory' not in result.stderr"

    - name: Cleanup cache devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item }}'
      with_items: "{{ cache_devices }}"

    - name: gather all device mapper entries
      shell: dmsetup ls
      register: dmsetup_ls_cmd

    - name: search for all remaining device mapper entries that belong to us
      set_fact:
        dm_entries: |
            {{ dmsetup_ls_cmd.stdout_lines | map('regex_findall', '^(?:VG|FASTVG)[0-9]*-\S+', '\\1') | map('join') | list  }}

    - name: Remove any residual device mapper entries
      shell: /bin/bash -c 'dmsetup remove --force {{ item }}'
      when: item != ''
      with_items: "{{ dm_entries }}"
