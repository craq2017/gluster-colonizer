- hosts: gluster_nodes
  become: yes
  any_errors_fatal: True

  tasks:

    - name: Build mntpath variable
      set_fact:
        mntpaths: "{{ mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"

    - name: Build arbiter_mntpath variable
      set_fact:
        arbiter_mntpaths: "{{ arbiter_mntpaths | default([]) }} + [{\"path\": \"/gluster/bricks/arbiter-{{ item.thinLV }}\", \"lv_path\": \"/dev/mapper/{{ item.vg }}-arbiter--{{ item.thinLV }}\"}]"
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - shell: /bin/bash -c 'echo "Cleaning up any existing Gluster configurations..." > {{ fifo }}'

    - name: Stop glusterd service
      systemd:
        name: glusterd
        state: stopped

    - name: Kill all Gluster processes
      shell: /bin/bash -c 'pkill -9 {{ item }}'
      with_items:
        - glusterfs
        - glusterfsd
      ignore_errors: yes

    - name: Clear existing Gluster configuration
      file:
        path: "/var/lib/glusterd/{{ item }}"
        state: absent
      with_items:
        - glusterd.info
        - glustershd
        - bitd
        - glusterfind
        - glustershd
        - nfs
        - peers
        - quotad
        - scrub
        - snaps
        - ss_brick
        - vols
      ignore_errors: yes

    - name: Clear any residual Gluster temporary files
      file:
        path: "{{ item }}"
        state: absent
      with_fileglob:
        - "/var/run/gluster/*"
      ignore_errors: yes

    - name: Unmount data bricks
      mount:
        name: "{{ item.0.path }}"
        state: "{{ item.1 }}"
      with_nested:
        - "{{ mntpaths }}"
        - ['absent', 'unmounted']

    - name: Unmount arbiter bricks
      mount:
        name: "{{ item.0.path }}"
        state: "{{ item.1 }}"
      with_nested:
        - "{{ arbiter_mntpaths }}"
        - ['absent', 'unmounted']
      when: arbiter == True

      #- name: Deactivate VGs
      #shell: /bin/bash -c 'vgchange -a n --activationmode partial {{ item.vg }}'
      #with_items: "{{ backend_configuration }}"
      #register: result
      #failed_when: "result.rc != 0 and 'not found' not in result.stderr"

    # we have to try to delete all LVM volumes in reverse order in which they potentially were created before since a simple vgremove --force is not enough since the lvg module is not robust enough - some existing thinpools or VGs still survive this
    - name: Delete data logical volumes
      lvol:
        vg: "{{ item.vg }}"
        lv: "{{ item.thinLV }}"
        state: absent
        force: yes
      with_items: "{{ backend_configuration }}"

    - name: Delete arbiter logical volumes
      lvol:
        vg: "{{ item.vg }}"
        lv: "arbiter-{{ item.thinLV }}"
        force: yes
      with_items: "{{ backend_configuration }}"
      when: arbiter == True

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: Clear cache volume
      lvol:
        vg: "FAST{{ item.vg }}"
        lv: "cpool{{ item.id|int + 1 }}"
        state: "absent"
        force: yes
      with_items:
        - "{{ backend_configuration }}"

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: Clear cache volume group
      lvg:
        vg: "FAST{{ item.vg }}"
        state: absent
        force: yes
      with_items: "{{ backend_configuration }}"

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: Remove data thin pools
      lvol:
        vg: "{{ item.vg }}"
        lv: "{{ item.tp }}"
        force: yes
        state: absent
      with_items: "{{ backend_configuration }}"

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: Clear data volume groups
      lvg:
        vg: "{{ item.vg }}"
        state: absent
        force: yes
      with_items: "{{ backend_configuration }}"

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: gather all device mapper entries
      shell: dmsetup ls
      register: dmsetup_ls_cmd

    - name: search for all remaining device mapper entries that belong to us
      set_fact:
        dm_entries: |
            {{ dmsetup_ls_cmd.stdout_lines | map('regex_findall', '^(?:VG|FASTVG)[0-9]*-\S+', '\\1') | map('join') | list  }}

    - name: Remove any residual device mapper entries
      shell: /bin/bash -c 'dmsetup remove --force {{ item }}'
      when: item != ''
      with_items: "{{ dm_entries }}"

    - name: wait for udev to settle
      shell: udevadm settle --timeout=60
      changed_when: false

    - name: Remove PV
      shell: /bin/bash -c 'pvremove --force {{ item.device }}'
      register: result
      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
      with_items: "{{ backend_configuration }}"

    - name: Remove cache PV
      #FIXME: The 'p' is needed below for nvme drives, but should not be
      #       there for devices with normal /dev/sdX disk paths.
      shell: /bin/bash -c 'pvremove --force {{ item.0 }}p{{ item.1["id"]|int + 1 }}'
      register: result
      failed_when: "result.rc != 0 and 'not found' not in result.stderr"
      with_nested:
        - "{{ cache_devices }}"
        - "{{ backend_configuration }}"

    - name: Cleanup data devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item["device"] }}'
      with_items: "{{ backend_configuration }}"
      register: result
      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"

    - name: Cleanup cache devices
      shell: /bin/bash -c '/sbin/wipefs -af {{ item }}'
      with_items: "{{ cache_devices }}"
      register: result
      failed_when: "result.rc != 0 and 'No such file or directory' not in result.stderr"
